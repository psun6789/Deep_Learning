{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1_3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y44yQFS-5uSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a1b381d-b689-452a-e4f0-1c9db49406e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nName : Peter Sunny Shanthveer Markappa\\nStudent Number: R00208303\\nAssignemnt : 01\\nSubject: Deep Learning\\nSubmission Date: 03- April - 2022\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "Name : Peter Sunny Shanthveer Markappa\n",
        "Student Number: R00208303\n",
        "Assignemnt : 01\n",
        "Subject: Deep Learning\n",
        "Submission Date: 03- April - 2022\n",
        "'''\n",
        "# mini batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from numpy import exp\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijzm-OyI5yJK",
        "outputId": "a2cb838b-eff9-4c4d-d400-f36d823b9891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import concat\n",
        "\n",
        "def softmax(vector):\n",
        "    e = tf.exp(vector, name ='exp')\n",
        "    return e / tf.math.reduce_sum(e, axis=None, keepdims=1, name=None)\n",
        "\n",
        "\n",
        "def forward_pass(x, w1, b1, w2, b2, w3, b3):\n",
        "    # We need to mutliply each training example by the weights and add bias\n",
        "    y_pred = tf.matmul(x, tf.transpose(w1)) + b1\n",
        "\n",
        "    relu_res = tf.keras.activations.relu(y_pred)\n",
        "    # print(\"relu_res = \", relu_res.shape)\n",
        "\n",
        "    # --------- Drop out layer ----------\n",
        "    probThreshold = 1 - 0.5\n",
        "    neuronsize = relu_res.shape[0]\n",
        "    trainingsize = relu_res.shape[1]\n",
        "\n",
        "    dropmatrix = tf.cast(tf.Variable(tf.random.uniform([neuronsize, trainingsize], dtype=tf.dtypes.float64)) < probThreshold, tf.float64)\n",
        "    relu_res = relu_res * dropmatrix\n",
        "    # --------- Drop out layer\n",
        "\n",
        "    y_pred1 = tf.matmul(relu_res, tf.transpose(w2)) + b2\n",
        "    relu_res1 = tf.keras.activations.relu(y_pred1)\n",
        "    # print(\"relu_res1 = \", relu_res1.shape)\n",
        "\n",
        "\n",
        "    y_pred2 = tf.matmul(relu_res1,tf.transpose(w3)) + b3\n",
        "    act = softmax(y_pred2)\n",
        "    # act = tf.keras.activations.softmax(y_pred1)\n",
        "    \n",
        "    return act\n",
        "\n",
        "\n",
        "def calculate_accuracy(y_pred, tr_y):\n",
        "    # Round the predictions by the logistical unit to either 1 or 0\n",
        "    predictions = tf.round(y_pred)\n",
        "    \n",
        "    # tf.equal will return a boolean array: True if prediction correct, False otherwise\n",
        "    # tf.cast converts the resulting boolean array to a numerical array \n",
        "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
        "    predictions_correct = tf.cast(tf.equal(tf.transpose(predictions), tr_y), tf.dtypes.float64)\n",
        "    \n",
        "    # Finally, we just determine the mean value of predictions_correct\n",
        "    accuracy = tf.reduce_mean(predictions_correct)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def cross_entropy(tr_y, y_pred):\n",
        "    return (- 1 / tr_y.shape[0]) * tf.math.reduce_sum(tr_y * tf.math.log(y_pred) + (1 - tr_y) * (tf.math.log(1 - y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # load the training and test data    \n",
        "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "\n",
        "    # reshape the feature data\n",
        "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "\n",
        "    # noramlise feature data\n",
        "    tr_x = tr_x / 255.0\n",
        "    te_x = te_x / 255.0\n",
        "\n",
        "#     print( \"Shape of training features \", tr_x.shape) \n",
        "#     print( \"Shape of test features \", te_x.shape) \n",
        "\n",
        "\n",
        "    # one hot encode the training labels and get the transpose\n",
        "    tr_y = np_utils.to_categorical(tr_y, 10)\n",
        "    tr_y = tr_y.T\n",
        "#     print (\"Shape of training labels after transpose \", tr_y.shape)\n",
        "\n",
        "    # one hot encode the test labels and get the transpose\n",
        "    te_y = np_utils.to_categorical(te_y, 10)\n",
        "    te_y = te_y.T\n",
        "#     print (\"Shape of testing labels after transpose \", te_y.shape)\n",
        "    \n",
        "    \n",
        "      # array is used to store change in cost function for each iteration of GD\n",
        "    trainingLoss= []\n",
        "    validationLoss= []\n",
        "    trainingAccuracies = []\n",
        "    validationAccuracies = []\n",
        "\n",
        "\n",
        "    \n",
        "    learning_rate = 0.01\n",
        "    num_Iterations = 500\n",
        "    \n",
        "    adam_optimizer = tf.keras.optimizers.Adam()\n",
        "#     cross_entropy = tf.keras.losses.categorical_crossentropy()\n",
        "    \n",
        "    \n",
        "    # #  X_digits, y_digits = loadData()    \n",
        "    # tr_x = tf.convert_to_tensor(tr_x, tf.dtypes.float64)\n",
        "    te_x = tf.convert_to_tensor(te_x, tf.dtypes.float64)\n",
        "    # tr_y = tf.convert_to_tensor(tr_y, tf.dtypes.float64)\n",
        "    te_y = tf.convert_to_tensor(te_y, tf.dtypes.float64)\n",
        "\n",
        "    #  X_digits, y_digits = loadData()     \n",
        "\n",
        "    w1 = tf.Variable(tf.random.normal( [300, 784] , mean=0.0, stddev=0.05,dtype=tf.dtypes.float64))\n",
        "    b1 = tf.Variable(tf.random.normal([300], dtype=tf.dtypes.float64))   \n",
        "\n",
        "    w2 = tf.Variable(tf.random.normal([100, 300] , mean=0.0, stddev=0.05,dtype=tf.dtypes.float64))\n",
        "    b2 = tf.Variable(tf.random.normal([100], dtype=tf.dtypes.float64))\n",
        "\n",
        "    w3 = tf.Variable(tf.random.normal([10, 100] , mean=0.0, stddev=0.05,dtype=tf.dtypes.float64))\n",
        "    b3 = tf.Variable(tf.random.normal([10], dtype=tf.dtypes.float64))\n",
        "\n",
        "\n",
        "    # Concatenate the training data and its labels before splitting\n",
        "    transpose_y = tr_y.T\n",
        "    feature_x_y = np.concatenate((tr_x, transpose_y), axis=1)\n",
        "    print(feature_x_y.shape)\n",
        "\n",
        "    batch_size = 10\n",
        "\n",
        "    shuffl_data = np.random.shuffle(feature_x_y)\n",
        "    each_batch = feature_x_y.shape[0] / batch_size\n",
        "    print(\"Each Batch Size\", each_batch)\n",
        "\n",
        "    # Iterate our training loop\n",
        "    for i in range(num_Iterations):\n",
        "      \n",
        "      for j in range(batch_size):\n",
        "        batch_data = feature_x_y[j*int(each_batch) : (j* int(each_batch)) + int(each_batch), :]\n",
        "        train_x = batch_data [ : , : 784]\n",
        "        train_y = batch_data [ : , 784:]\n",
        "\n",
        "        train_x = tf.convert_to_tensor(train_x, tf.dtypes.float64)\n",
        "        train_y = tf.convert_to_tensor(train_y, tf.dtypes.float64)\n",
        "\n",
        "        # Create an instance of GradientTape to monitor the forward pass\n",
        "        # and calcualte the gradients for each of the variables m and c\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # y_pred = forward_pass(tr_x, w1, b1, w2, b2)\n",
        "            y_pred = forward_pass(train_x, w1, b1, w2, b2, w3, b3)\n",
        "\n",
        "            currentLoss = cross_entropy(train_y, y_pred)\n",
        "            # print(\"Current Loss\", currentLoss)\n",
        "            trainingLoss.append(currentLoss)\n",
        "        \n",
        "        gradients = tape.gradient(currentLoss, [w1, b1, w2, b2, w3, b3])\n",
        "\n",
        "        accuracy = calculate_accuracy(y_pred, tf.transpose(train_y))\n",
        "\n",
        "        \n",
        "        adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
        "        \n",
        "        if i % 50 == 0:\n",
        "          print (\"Iteration \", i, \": Loss = \",currentLoss.numpy(), \"  Acc: \", accuracy.numpy())\n",
        "\n",
        "    y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3)\n",
        "    currentLoss = cross_entropy(te_y, tf.transpose(y_pred))\n",
        "    test_accuracy = calculate_accuracy(y_pred, te_y) \n",
        "    print (\"Test Accuracy : \", test_accuracy.numpy())\n",
        "\n",
        "    \n",
        "    # plt.plot(validationLoss, label=\"Val Loss\")\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(validationAccuracies, label=\"Val Acc\")\n",
        "    # plt.show()\n",
        "\n",
        "    plt.plot(trainingLoss, label=\"Train Loss\")\n",
        "    plt.plot(trainingAccuracies, label=\"Train Acc\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fuwtixlv5yMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8npCrAIU5yPO",
        "outputId": "765ae6a6-f200-420f-8564-92599a13c684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 794)\n",
            "Each Batch Size 6000.0\n",
            "Iteration  0 : Loss =  11.59743748374064   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.404590076653733   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.301386910214626   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.215093256245135   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.128400445614261   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.07504510082249   Acc:  0.9\n",
            "Iteration  0 : Loss =  11.003306745968683   Acc:  0.9\n",
            "Iteration  0 : Loss =  10.986884374212126   Acc:  0.9\n",
            "Iteration  0 : Loss =  10.93588931925673   Acc:  0.9\n",
            "Iteration  0 : Loss =  10.897523289297755   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.05315610300112   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.045719169997389   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.04556144928337   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.046734646499992   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.073301744820409   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.062365878545076   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.044820993566047   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.056460710004856   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.050334680124147   Acc:  0.9\n",
            "Iteration  50 : Loss =  9.061521840829197   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.982857259462056   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.978719512810883   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.97857155372212   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.979615910795633   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.995726685820943   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.99126625761022   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.97804659007227   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.9824484393417   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.985635904832955   Acc:  0.9\n",
            "Iteration  100 : Loss =  8.990023541456441   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.932411425754331   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.932986346031667   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.929006667631123   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.934518153753265   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.952280932587472   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.942736158336059   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.936089684640013   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.9413652924085   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.933513034194243   Acc:  0.9\n",
            "Iteration  150 : Loss =  8.943243589017243   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.897399148999218   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.89751141119326   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.896224660054203   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.8974102317618   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.92197959926681   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.90783931960263   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.901504318605943   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.908726500456396   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.900026010084247   Acc:  0.9\n",
            "Iteration  200 : Loss =  8.90772772990948   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.864592778362313   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.87018408458203   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.868086055677551   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.873052585997854   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.882688743061156   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.880133751754112   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.87720759443245   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.876915335056143   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.870464248784883   Acc:  0.9\n",
            "Iteration  250 : Loss =  8.88697232909124   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.853393579170694   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.851246857898909   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.85702927451954   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.854746222309128   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.870622237329155   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.856350188212376   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.85601148568906   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.859905563601304   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.853313846171618   Acc:  0.9\n",
            "Iteration  300 : Loss =  8.857675800131243   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.836581790913442   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.837432090092493   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.834069345777953   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.841846744904178   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.849244938116534   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.849076556200771   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.839702158235424   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.84377991732543   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.838041019965479   Acc:  0.9\n",
            "Iteration  350 : Loss =  8.84379912673074   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.82063198251732   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.827365791811904   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.819277920502774   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.824544606748102   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.830862236989045   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.828172723906079   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.834269953565757   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.829477086243406   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.820464134105654   Acc:  0.9\n",
            "Iteration  400 : Loss =  8.827166811137426   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.804783547109658   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.810029095014528   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.808842419921138   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.816823072532868   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.823670645745585   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.811742436333969   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.814520444134889   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.814341804476081   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.816367454559357   Acc:  0.9\n",
            "Iteration  450 : Loss =  8.815139536064727   Acc:  0.9\n",
            "Test Accuracy :  0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}